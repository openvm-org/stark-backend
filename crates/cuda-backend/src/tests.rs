//! Tests for the CUDA backend.
//!
//! Shared tests are generated by the [`openvm_backend_tests::backend_test_suite!`] macro.
//! GPU-specific tests (sumcheck, monomial vs DAG, stacked reduction) remain here.
//! Additional GPU-specific test cases for parameterized shared tests are added below.

use itertools::Itertools;
use openvm_stark_backend::{
    prover::{
        stacked_pcs::stacked_commit,
        stacked_reduction::{prove_stacked_opening_reduction, StackedReductionCpu},
        DeviceDataTransporter, MatrixDimensions, MultiRapProver,
    },
    test_utils::{default_test_params_small, FibFixture, TestFixture},
    verifier::{
        stacked_reduction::{verify_stacked_reduction, StackedReductionError},
        sumcheck::{verify_sumcheck_multilinear, verify_sumcheck_prismalinear},
    },
    FiatShamirTranscript, StarkEngine, StarkProtocolConfig,
};
use openvm_stark_sdk::{
    config::baby_bear_poseidon2::{
        default_duplex_sponge, BabyBearPoseidon2CpuEngine, DuplexSponge,
    },
    utils::setup_tracing_with_log_level,
};
use p3_field::{PrimeCharacteristicRing, PrimeField32, TwoAdicField};
use rand::{rngs::StdRng, Rng, SeedableRng};
use test_case::test_case;
use tracing::{debug, Level};

use crate::{
    prelude::{EF, F, SC},
    sponge::DuplexSpongeGpu,
    sumcheck::{sumcheck_multilinear_gpu, sumcheck_prismalinear_gpu},
    BabyBearPoseidon2GpuEngine,
};

type Engine = BabyBearPoseidon2GpuEngine;

// ===========================================================================
// Shared test suite (engine-generic + WHIR)
// ===========================================================================

openvm_backend_tests::backend_test_suite!(Engine);

// ===========================================================================
// GPU-specific tests (not shared)
// ===========================================================================

#[test]
fn test_plain_multilinear_sumcheck() -> Result<(), String> {
    let n = 15;
    let mut rng = StdRng::from_seed([228; 32]);

    let num_pts = 1 << n;
    assert!((F::ORDER_U32 - 1) % num_pts == 0);

    let evals = (0..num_pts)
        .map(|_| F::from_u32(rng.random_range(0..F::ORDER_U32)))
        .collect::<Vec<_>>();
    let mut prover_sponge_gpu = DuplexSpongeGpu::default();
    let mut verifier_sponge = default_duplex_sponge();

    let (proof_gpu, _) = sumcheck_multilinear_gpu(&mut prover_sponge_gpu, &evals);

    verify_sumcheck_multilinear::<SC, _>(&mut verifier_sponge, &proof_gpu)
}

#[test]
fn test_plain_prismalinear_sumcheck() -> Result<(), String> {
    let n = 5;
    let l_skip = 10;
    let mut rng = StdRng::from_seed([228; 32]);

    let dim = n + l_skip;
    let num_pts = 1 << dim;
    assert!((F::ORDER_U32 - 1) % num_pts == 0);

    let evals = (0..num_pts)
        .map(|_| F::from_u32(rng.random_range(0..F::ORDER_U32)))
        .collect::<Vec<_>>();

    let mut prover_sponge = DuplexSpongeGpu::default();
    let mut verifier_sponge = default_duplex_sponge();

    let (proof, _) = sumcheck_prismalinear_gpu(&mut prover_sponge, l_skip, &evals);
    verify_sumcheck_prismalinear::<SC, _>(&mut verifier_sponge, l_skip, &proof)
}

#[test_case(9)]
#[test_case(2 ; "when log_height equals l_skip")]
#[test_case(1 ; "when log_height less than l_skip")]
#[test_case(0 ; "when log_height is zero")]
fn test_stacked_opening_reduction(
    log_trace_degree: usize,
) -> Result<(), StackedReductionError<EF>> {
    setup_tracing_with_log_level(Level::DEBUG);

    let gpu_engine = BabyBearPoseidon2GpuEngine::new(default_test_params_small());
    let params = gpu_engine.config().params().clone();

    let engine = BabyBearPoseidon2CpuEngine::<DuplexSponge>::new(params.clone());
    let fib = FibFixture::new(0, 1, 1 << log_trace_degree);
    let (pk, _vk) = fib.keygen(&engine);
    let pk = engine.device().transport_pk_to_device(&pk);
    let mut ctx = fib.generate_proving_ctx();

    ctx.sort_for_stacking();

    let (_, common_main_pcs_data) = {
        stacked_commit(
            engine.config().hasher(),
            params.l_skip,
            params.n_stack,
            params.log_blowup,
            params.k_whir(),
            &ctx.common_main_traces()
                .map(|(_, trace)| trace)
                .collect_vec(),
        )
    };

    let omega_skip = F::two_adic_generator(params.l_skip);
    let omega_skip_pows = omega_skip.powers().take(1 << params.l_skip).collect_vec();

    let device = engine.device();
    let ((_, batch_proof), r) = device.prove_rap_constraints(
        &mut DuplexSpongeGpu::default(),
        &pk,
        &ctx,
        &common_main_pcs_data,
    );

    let need_rot = pk.per_air[ctx.per_trace[0].0].vk.params.need_rot;
    let need_rot_per_commit = vec![vec![need_rot]];
    let (stacking_proof, _) = prove_stacked_opening_reduction::<SC, _, _, _, StackedReductionCpu<SC>>(
        device,
        &mut DuplexSpongeGpu::default(),
        params.n_stack,
        vec![&common_main_pcs_data],
        need_rot_per_commit.clone(),
        &r,
    );

    debug!(?batch_proof.column_openings);

    let u_prism = verify_stacked_reduction(
        &mut default_duplex_sponge(),
        &stacking_proof,
        &[common_main_pcs_data.layout],
        &need_rot_per_commit,
        params.l_skip,
        params.n_stack,
        &batch_proof.column_openings,
        &r,
        &omega_skip_pows,
    )?;
    assert_eq!(u_prism.len(), params.n_stack + 1);
    Ok(())
}

/// Tests that monomial-based and DAG-based zerocheck evaluation paths produce identical results.
#[test]
fn test_monomial_vs_dag_equivalence() {
    use openvm_cuda_common::copy::{MemCopyD2H, MemCopyH2D};
    use openvm_stark_backend::{
        poly_common::eval_eq_uni_at_one, test_utils::prove_up_to_batch_constraints,
    };
    use p3_util::log2_strict_usize;

    use crate::{
        cuda::logup_zerocheck::{fold_selectors_round0, interpolate_columns_gpu, MainMatrixPtrs},
        logup_zerocheck::{
            batch_mle::{TraceCtx, ZerocheckMleBatchBuilder},
            batch_mle_monomial::{compute_lambda_combinations, ZerocheckMonomialBatch},
            fold_ple::fold_ple_evals_rotate,
        },
        poly::EqEvalSegments,
        prelude::EF,
    };

    setup_tracing_with_log_level(Level::DEBUG);

    let log_trace_degree = 5;
    let threshold = 32u32;

    let engine = BabyBearPoseidon2GpuEngine::new(default_test_params_small());
    let device = engine.device();
    let params = engine.params();
    let l_skip = params.l_skip;

    let fib = FibFixture::new(0, 1, 1 << log_trace_degree);
    let (pk, _vk) = fib.keygen(&engine);
    let pk = device.transport_pk_to_device(&pk);

    let ctx_for_challenges = device
        .transport_proving_ctx_to_device(&fib.generate_proving_ctx())
        .into_sorted();
    let mut prover_sponge = DuplexSpongeGpu::default();
    let ((_, _), r) =
        prove_up_to_batch_constraints(&engine, &mut prover_sponge, &pk, ctx_for_challenges);

    let ctx = device
        .transport_proving_ctx_to_device(&fib.generate_proving_ctx())
        .into_sorted();

    let height = ctx.per_trace[0].1.common_main.height();
    let n_calc = log2_strict_usize(height).saturating_sub(l_skip);
    let xi_len = l_skip + n_calc + 1;

    let mut xi: Vec<EF> = Vec::with_capacity(xi_len);
    for _ in 0..l_skip {
        xi.push(prover_sponge.sample_ext());
    }
    xi.extend_from_slice(&r);
    while xi.len() < xi_len {
        xi.push(prover_sponge.sample_ext());
    }
    assert!(xi.len() > l_skip, "xi vector must have enough elements");

    let omega_skip = F::two_adic_generator(l_skip);
    let omega_skip_pows: Vec<F> = omega_skip.powers().take(1 << l_skip).collect();
    let d_omega_skip_pows = omega_skip_pows.to_device().unwrap();

    let (air_idx, air_ctx) = &ctx.per_trace[0];
    let height = air_ctx.common_main.height();
    let n = log2_strict_usize(height) as isize - l_skip as isize;
    let n_lift = n.max(0) as usize;

    let eq_xis = EqEvalSegments::new(&xi[l_skip..]).expect("failed to compute eq_xis");

    let sel_height = 1 << n_lift;
    let mut sel_cols = F::zero_vec(3 * sel_height);
    sel_cols[sel_height..2 * sel_height - 1].fill(F::ONE);
    sel_cols[0] = F::ONE;
    sel_cols[2 * sel_height + sel_height - 1] = F::ONE;
    let d_sels_base = sel_cols.to_device().unwrap();

    let (l, r_fold) = if n.is_negative() {
        (
            l_skip.wrapping_add_signed(n),
            r[0].exp_power_of_2(-n as usize),
        )
    } else {
        (l_skip, r[0])
    };
    let omega = F::two_adic_generator(l);
    let is_first = eval_eq_uni_at_one(l, r_fold);
    let is_last = eval_eq_uni_at_one(l, r_fold * omega);
    let d_sels_folded =
        openvm_cuda_common::d_buffer::DeviceBuffer::<EF>::with_capacity(sel_height * 3);
    unsafe {
        fold_selectors_round0(
            d_sels_folded.as_mut_ptr(),
            d_sels_base.as_ptr(),
            is_first,
            is_last,
            sel_height,
        )
        .unwrap();
    }

    let inv_lagrange_denoms_r0 =
        crate::utils::compute_barycentric_inv_lagrange_denoms(l_skip, &omega_skip_pows, r[0]);
    let d_inv_lagrange_denoms_r0 = inv_lagrange_denoms_r0.to_device().unwrap();

    let mat_folded = fold_ple_evals_rotate(
        l_skip,
        &d_omega_skip_pows,
        &air_ctx.common_main,
        &d_inv_lagrange_denoms_r0,
        true,
    )
    .unwrap();

    let lambda = prover_sponge.sample_ext();
    let air_pk = &pk.per_air[*air_idx];
    let max_num_constraints = air_pk
        .vk
        .symbolic_constraints
        .constraints
        .constraint_idx
        .len();
    let h_lambda_pows: Vec<EF> = lambda.powers().take(max_num_constraints).collect();
    let d_lambda_pows = h_lambda_pows.to_device().unwrap();

    let d_public_values = if air_ctx.public_values.is_empty() {
        openvm_cuda_common::d_buffer::DeviceBuffer::new()
    } else {
        air_ctx.public_values.to_device().unwrap()
    };

    let dag = &air_pk.vk.symbolic_constraints;
    let has_constraints = dag.constraints.num_constraints() > 0;
    assert!(has_constraints, "FibFixture should have constraints");

    let has_monomials = air_pk
        .other_data
        .zerocheck_monomials
        .as_ref()
        .map(|m| m.num_monomials > 0)
        .unwrap_or(false);
    assert!(
        has_monomials,
        "Proving key should have expanded monomials for monomial path"
    );

    let s_deg = params.max_constraint_degree as usize + 1;

    for test_round in 1..=n_lift.min(3) {
        let n_round = n_lift.saturating_sub(test_round - 1);
        let test_height = 1 << n_round;
        let num_y = (test_height / 2) as u32;

        if num_y == 0 || num_y > threshold {
            continue;
        }

        debug!(test_round, num_y, %threshold, "testing monomial vs DAG equivalence");

        let has_interactions = false;
        let mut columns: Vec<*const EF> = Vec::new();
        columns.push(eq_xis.get_ptr(n_round));
        for col in 0..3 {
            columns.push(d_sels_folded.as_ptr().wrapping_add(col * sel_height));
        }
        for col in 0..mat_folded.width() {
            columns.push(
                mat_folded
                    .buffer()
                    .as_ptr()
                    .wrapping_add(col * mat_folded.height()),
            );
        }

        let interpolated =
            crate::base::DeviceMatrix::<EF>::with_capacity(s_deg * num_y as usize, columns.len());
        let d_columns = columns.to_device().unwrap();
        unsafe {
            interpolate_columns_gpu(interpolated.buffer(), &d_columns, s_deg, num_y as usize)
                .expect("failed to interpolate columns");
        }

        let interpolated_height = interpolated.height();
        let eq_xi_ptr = eq_xis.get_ptr(n_round);
        let sels_ptr = interpolated
            .buffer()
            .as_ptr()
            .wrapping_add(interpolated_height);

        let main_ptrs = [MainMatrixPtrs {
            data: interpolated
                .buffer()
                .as_ptr()
                .wrapping_add(4 * interpolated_height),
            air_width: mat_folded.width() as u32 / 2,
        }];
        let main_ptrs_dev = main_ptrs.to_device().unwrap();

        let trace_ctx = TraceCtx {
            trace_idx: 0,
            air_idx: *air_idx,
            n_lift,
            num_y,
            has_constraints: true,
            has_interactions,
            norm_factor: F::ONE,
            eq_xi_ptr,
            sels_ptr,
            prep_ptr: MainMatrixPtrs {
                data: std::ptr::null(),
                air_width: 0,
            },
            main_ptrs_dev,
            public_ptr: d_public_values.as_ptr(),
            eq_3bs_ptr: std::ptr::null(),
        };

        let dag_builder =
            ZerocheckMleBatchBuilder::new(std::iter::once(&trace_ctx), &pk, s_deg as u32);
        let dag_output = dag_builder.evaluate(&d_lambda_pows, s_deg as u32);
        let dag_results: Vec<EF> = dag_output.to_host().expect("copy DAG output");

        let lambda_comb = compute_lambda_combinations(&pk, 0, &d_lambda_pows).unwrap();
        let mono_batch =
            ZerocheckMonomialBatch::new(std::iter::once(&trace_ctx), &pk, &[&lambda_comb]);
        let mono_output = mono_batch.evaluate(s_deg as u32);
        let mono_results: Vec<EF> = mono_output.to_host().expect("copy monomial output");

        assert_eq!(
            dag_results.len(),
            mono_results.len(),
            "Output lengths should match"
        );
        for (i, (dag_val, mono_val)) in dag_results.iter().zip(mono_results.iter()).enumerate() {
            assert_eq!(
                dag_val, mono_val,
                "Mismatch at index {i} for num_y={num_y}: DAG={dag_val:?}, monomial={mono_val:?}"
            );
        }
        debug!(
            num_y,
            num_results = dag_results.len(),
            "monomial vs DAG equivalence verified"
        );
    }
}
